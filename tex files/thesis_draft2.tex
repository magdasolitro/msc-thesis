\documentclass[12pt,a4paper]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsmath,mathtools,breqn,amsfonts, amsthm, mathrsfs, amssymb}
\usepackage[margin=3cm,letterpaper]{geometry} % decreases margins
\usepackage{comment}
\usepackage{hyperref}
\usepackage{color}
\usepackage{listings}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{graphicx,caption}
\usepackage[toc,page]{appendix}
\usepackage{algorithm, algpseudocode}

\definecolor{green}{RGB}{60,179,113}
\definecolor{violet}{RGB}{153,50,204}
\definecolor{blue}{RGB}{65,105,225}
\definecolor{gray}{RGB}{128,128,128}
\definecolor{red}{RGB}{255,100,100}
\definecolor{lightgreen}{RGB}{141,213,161}
\definecolor{lightpurple}{RGB}{215,189,226}
\definecolor{lightgreen}{RGB}{171,235,198}
\definecolor{yellow}{RGB}{249,231,159}
\definecolor{pink}{RGB}{253,175,172}
\definecolor{lightblue}{RGB}{174,214,241}

% set up the colors for different kind of references
\hypersetup{
	colorlinks = true,
	linkcolor  = violet,
	citecolor  = green 
}

\lstset{
	language=C,
	aboveskip=3mm,
	belowskip=3mm,
	xleftmargin=5mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=left,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	morekeywords={bool},
	commentstyle=\color{green},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4
}

\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{\tiny def}}}{=}}

\newcommand{\st}{\hspace{2px}.\hspace{2px}}

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\raggedbottom

\begin{document}
	%\frontmatter
	
	\pagenumbering{gobble}
	\begin{titlepage}
		\begin{center}
			\vspace*{1cm}
			
			\Huge
			\textbf{\Huge Detecting Microarchitectural Attacks}
			
			\vspace{0.3cm}
			\LARGE \textsc{Draft}
			
			\vspace{1.5cm}
			
			\large Author: \textit{Magdalena M. Solitro}
			
			\vfill
		\end{center}
	\end{titlepage}
	
	\tableofcontents
	
	\mainmatter

	\chapter{Introduction}\label{chapter:intro}
	
	
	\chapter{Static analysis for cybersecurity}\label{chapter:static_analysis}
	Static analysis is a set of automated techniques used to inspect a program to check whether it satisfies certain properties, without executing it. The origins of static analysis date back to the Seventies and it was initially developed to speed up the compilation process. Nowadays, these techniques are used for a multitude of purposes, such as program verification, synthesis of optimised code, and certification of critical software applications \cite{Rival2020}. 
	
	This chapter will provide an insight on the theoretical principles and practical approaches of static analysis, with particular attention to the cybersecurity perspective. Some of the question concerning safety and security that we can tackle with static analysis are, for example: will the software crash under certain circumstances? Will specific inputs give illicit access to information that shouldn't be disclosed? Does the software accomplish \textit{exactly} what it is programmed for or will there be undesired side effects? We can try to give an answer to these questions through intensive testing, but this approach is destined to be a coarse approximation of the correct answer: the domain of possible inputs that need to be examined is often far too broad to be tested in a reasonable amount of time and selecting a small subset of inputs is likely to leave out those few that can cause a malfunctioning. In addition to that, nowadays computer programs are behind the correct functioning of safety-critical systems, such as industrial machinery or avionics: in these contexts, where unexpected behaviours can likely cause serious injuries or even death, the analysis of the program cannot be left to inaccurate methods. These are just a couple of reasons that highlight the necessity of techniques that allow us to verify quickly and precisely the properties of a program.
	
	We begin our dive into static analysis by firstly providing the theoretical arguments on which this field is based and then we will move on to describe some modern strategies to carry out the analysis.
	\section{Theoretical foundations}
	The goals of static analysis are ambitious, because it aims to provide tools that can deliver the following advantages:
	\begin{itemize}
		\item \textbf{Speed:} the possibility of using methods to analyse code quickly implies being able to perform more controls, more frequently; 
		\item \textbf{Automation:} being able to check the properties of a program automatically relieve humans from a cumbersome and error-prone process;
		\item \textbf{Precision:} static analysis can detect flaws that appear only very rarely during execution, and thus would be hard to spot through simple testing.
	\end{itemize}
	These objectives can be partially achieved, although not entirely:  static analysis is not infallible, which means that it does not always provide the correct results. To be more precise, the effectiveness of static analysis has to confront some intrinsic limits of computation, namely the undecidability of the \textit{Halting problem} and \textit{Rice's theorem}. 
	\theoremstyle{theorem}
	\newtheorem{thm}{Theorem}
	\begin{thm}[Halting problem] Let $\mathcal{L}$ be the set of all programs that can be written in a certain language, and let $p$ be one of such programs. 
		
		The halting problem consists in finding an algorithm \texttt{halt} such that,  
		\[
		\forall\hspace{2px}p\in\mathcal{L}\st\texttt{halt(p)} = true \Longleftrightarrow p\text{ terminates}
		\]
		The halting problem is not computable: an algorithm such as \texttt{halt} does not exist, as proved simultaneously by Alonso Church \cite{Church1936} and Alan Turing \cite{Turing1937} in 1936.
	\end{thm}
	This theorem provide an example of something that a program analysis tool cannot detect: no algorithm can decide whether a program, written in any language, will terminate or not.
	
	There is another theoretical result, that provides an even stronger statement on what we can prove about the properties of a program:
	\begin{thm}[Rice's Theorem] Let $\mathcal{L}$ be a Turing-complete language, and let $\mathcal{P}$ be a nontrivial semantic property of programs of $\mathcal{L}$. There exists no algorithm \texttt{SatP} such that,
		\[
		\forall\hspace{2px}p\in\mathcal{L}\st\texttt{SatP(p)} = true \Longleftrightarrow p\text{ satisfies the semantic property } \mathcal{P}. 	
		\]
	\end{thm}
	The notion of "nontrivial" mentioned in the theorem identifies those properties that either concern \textit{every} program in the language, or \textit{none}, therefore
	\[
	\mathcal{P}\text{ is trivial} \Longleftrightarrow \mathcal{P} = \mathcal{L} \vee \mathcal{P} = \emptyset
	\]
	Clearly, we are not interested in trivial properties, because there is really nothing to prove about them in a program!
	Intuitively, what Rice's theorem states is that, for \textit{any interesting property}, we cannot have an algorithm that is able to decide whether a certain program has that property or not. This sounds discouraging: the theorems mentioned above seem to destroy any hope of being able to prove anything interesting about programs. Luckily, this is not quite the case: while it is impossible to have an automated procedure that correctly detects a certain property in \textit{all} cases, it is perfectly feasible that it does it \textit{sometimes}. This also means that it's impossible to construct a "perfect" static analysis tool, namely one that is always able to detect any (extensional) property of a program: this justifies the existance of a research field dedicated to static analysis.
	
	What we just said implies that the results provided by the algorithm will occasionally produce unreliable results, such as false positives and false negatives: the former refers to the detection of bugs that the program doesn't actually have, while the second one concern the failure of finding bugs that the code indeed has. False negatives are clearly a much bigger issue, because they lead to a false sense of security. For this reason, a good tool for static analysis should never fail to detect bugs, while it is allowed to output a false positive \cite{Gomes2009}. Another way around the limitation of Rice's Theorem is to give up complete automation, designing tools that require human intervention to compute the final result, accepting the risk of introducing mistakes in the computation of the final result.
	
	The desiderable properties of a program analysis tools can be formalised by means of two notions: \textit{soundness} and \textit{completeness}.
	
	\begin{defn}[Soundness]
		Let \texttt{analyse} be a program that tests whether another program has a certain property $\mathcal{P}$ and let $\mathcal{L}$ be a programming language. We say that the program \texttt{analyse} is \textbf{sound} with respect to $\mathcal{P}$ if the following condition is satisfied:
		\[
		\forall p \in \mathcal{L} \st \text{\texttt{analyse}}(p) = \text{\textbf{true}} \Longrightarrow p \text{ satisfies } \mathcal{P}
		\]
	\end{defn}
	In simple terms, a proof system is sound if all the statements it proves are actually true. A trivial \texttt{analysis} program that is guaranteed to be sound is the one that always returns false: by invalidating the premise, it makes the overall implication to be true, even though such analyser would clearly be of no utility.
	
	For the definition of completeness, we stick to the same notation.
	\begin{defn}[Completeness]
		We say that a program \texttt{analyse} is \textit{complete} with respect to $\mathcal{P}$ if the following condition is satisfied:
		\[
		\forall p \in \mathcal{L} \st p \text{ satisfies } \mathcal{P} \Longrightarrow \text{\texttt{analyse}}(p) = \text{\textbf{true}} 
		\]
	\end{defn}
	As the definition suggests, a \textit{complete} analyser is one that is able to prove \textit{all} the properties that are satisfied by a program. More generally, a formal system is said to be complete with respect to a particular property, if every formula that satisfies that property is a theorem (i.e. can be proved) within the system.
	
	Also in this case, we can provide a trivial \texttt{analysis} program that is guaranteed to be complete, namely the one that always returns true. In fact, if the consequence is true, the overall implication will always evaluate to true, independently from the truth value of the premise.
	
	Soundness and completeness are two essential aspect of a correct analysis, but an inherent limit of computability prevents us from having both. This limit comes with Gödel's Incompleteness Theorems, which essentially show that there is an intrinsic gap in mathematics between \textit{proof} and \textit{truth}. These theorems are extremely complex and profound, and thus we will not dive into the details, but the interested readers can find out more about the topic in \cite{Odifreddi1989}, \cite{Rogers1987}, \cite{Hinman2007}.
	\section{Practical approaches and related challenges}
	This section discusses the general ideas behind some of the most effective approaches used by the tools discussed in Chapter \ref{chapter:verification}. The description should be considered as a high-level introduction whose aim is to give an idea of the mechanisms leveraged by these tools.
	\subsection{Taint analysis}\label{sec:taint}
	Taint analysis is a technique that involves labelling each variable supplied to the program by an untrusted source as \textit{tainted}, assuming that the untrusted source could be an adversary trying to attack the system.
	Then, every variable in the program whose value is dependent from tainted variables is also marked as tainted. In this way, we can keep track of the propagation of tainted data during the program execution and detect when this data is used in dangerous ways. Taint analysis can be used to identify parts of code vulnerable to Spectre: for instance, in a Spectre-PHT attack, certain values can be supplied repeatedly to a conditional branch to induce a misprediction. The following example shows a Spectre-PHT gadget where the tainted variables are marked in red:
	
	\begin{lstlisting}[escapechar=!]
int checkValue(int !\colorbox{red}{x}!){
	if(!\colorbox{red}{x}! < len(array1)){
		!\colorbox{red}{y}! = array2[array1[!\colorbox{red}{x}!] * 4096;
	}
	return y;
}
	\end{lstlisting}
	The parameter \texttt{x} is supplied by the user, therefore is marked as tainted. The value of \texttt{y}, determined in line 3, depends on \texttt{x}, thus \texttt{y} becomes automatically tainted. This is an example of taint (dynamic) analysis \textit{based on data flow}, because it involves marking external taint data and tracking its propagation throughout the execution.
	
	This type of analysis can be used for vulnerability detection \cite{Newsome2005}, malware analysis \cite{Bayer2009} \cite{Yin2007}, and web applications \cite{Balzarotti2008} \cite{NguyenTuong2005}, and can be applied both statically or dynamically.
	
	Dynamic taint analysis can be based either on \textit{data flow} (the example above is an instance of this approach) or on \textit{control flow}. The latter involves constructing a control flow graph (CFG) of the program, by examining the branching instructions that govern the execution of the code \cite{Dai2018}.
	
	\subsubsection{Challenges in taint analysis}
	The main issues that taint analysis encounters are \textbf{under-tainting}, \textbf{over-tainting} and \textbf{state space explosion}. Let's start with the first one: under-tainting means that the analysis tool does not mark as tainted a data that was should be marked as such. This happens when the data is not arithmetically derived by an untrusted input or, more generally, is not linked to such input by an explicit instruction in the program, but is still influenced by it. This phenomenon causes the detection of \textbf{false negatives}, namely the tool recognises a piece of code as secure, even though it's not. A clever example of how this can occur is proposed in \cite{Newsome2005}:
	\begin{lstlisting}
if(x==0){
	y=0;
} else if (x==1){
	y=1;
} else if (x==2){
	y=2;
} ...
	\end{lstlisting}
	This conditional structure goes on in the form
\begin{lstlisting}[mathescape=true]
if(x == $n$){
	y = $n$;
}
\end{lstlisting}
	This program is semantically equivalent to the following instruction:
	\begin{lstlisting}
y = x;
	\end{lstlisting}
	However, a taint analysis will mark \texttt{y} as tainted in the last instruction, but not in the first example. Under-tainting affects TaintCheck \cite{Newsome2005}, even though the authors proposed some mitigations to the problem.
	
	The opposite problem is \textbf{over-tainting}, which leads to the detection of false positives: a piece of code can be evaluated as insecure, even in the absence of any feasible execution that taints a certain data. As already stated previously in this chapter, false positives are preferred to false negatives, since the latter can give a false sense of security and fail to notify potential vulnerabilities. One source of over-tainting can be a conservative extraction of the Control Flow Graph (CFG), which is defined as follows:
	\begin{defn}[Control Flow Graph (CFG)]
		A \textit{control flow graph (CFG)} is a directed graph $G=(V,E)$ in which nodes represent the program's instructions, while an edge $u \rightarrow v$ represents a possible flow from the instruction $u$ to the instruction $v$. This means that there exists at least one run of the program in which the execution of $u$ is followed immediately by the execution of $v$. The set $V$ of nodes contains also two special nodes: \textit{START}, that has no predecessors and from which every node is reachable, and \textit{END}, which has no successors and is reachable from every node.
	\end{defn}
	
	The construction of an accurate graph from the binary is often challenging, due to the difficulty in determining the targets of indirect branches and function calls. For this reason, tools such as oo7 \cite{Wang2019} use a conservative approximation of the CFG, which can contain more edges that expected.
	
	The last issue worth mentioning is the \textbf{state space explosion} (also known as path explosion), which can occur during the construction of the CFG or the Control Dependency Graph (CDG). The latter is defined as follows: (TO BE REVIEWED)
	
	\begin{defn}[Control Dependency Graph (CDG)]
		A \textit{control dependency graph (CDG)} \cite{Krinke2007} is a directed graph $G=(V,E)$ in which nodes represent the program's statements or expressions, while an edge $x\rightarrow y$ expresses the fact that the statement $x$ assigns a variable which is used in $y$, which means that the mere execution of $y$ depends on the value of the expression $x$. 
	\end{defn}
	The CDG is used by the analyser to keep track of how the tainted data affects the other variables in the program. However, as the complexity of the program increases, the size of these graphs grows exponentially and can become infeasible to analyse. This issue is also encountered in another popular approach, symbolic execution, which is discussed in the below. 
	\subsection{Symbolic execution}\label{sec:symbolic-exec}
	One way to test the behaviour of a program is to provide it with different, random inputs and observe the result of every execution. However, this approach presents a glaring limit: the domain of all the possible inputs can be extremely wide, if not infinite, which makes it infeasible to try all of them. One way around this obstacle is to use symbolic execution \cite{Baldoni2018} \cite{King76}: instead of using fully specified input values, we simultaneously explore multiple paths that a program can take under different inputs. This is possible by representing those inputs that cannot be determined statically under a symbol, namely an abstract value. Such inputs could be, for example, paramenters that must be provided by the user at run time.
	
	During the symbolic execution of a program, the analysis engine keeps track the following information:
	\begin{itemize}
		\item The next \textit{statement} to evaluate (\texttt{stmt}).
		\item A \textit{symbolic store} $\sigma$, that maps the variables of the program either with expressions over concrete values or with symbolic values.
		\item Some \textit{path constraints} $\pi$, namely "a formula that expresses a set of assumptions on the symbols due to branches taken in the execution to reach \texttt{stmt}" \cite{Baldoni2018}.
	\end{itemize}
	Different execution paths distinguish themselves by the assumptions that are made about the symbolic values. To give an idea about how this process works, consider the following piece of code:
	\begin{lstlisting}
int func(int a){
	int x = 1;
	bool y = true;
	if (a != 0){
		...
	}
}
	\end{lstlisting} 
	At the beginning of the symbolic execution, the state maintained by the engine will be:
	\[
	(\textcolor{blue}{\texttt{stmt}}, \textcolor{violet}{\sigma}, \textcolor{green}{\pi}) = (\textcolor{blue}{\texttt{int x = 1}}, \textcolor{violet}{\sigma = \{a \mapsto \alpha_a\}}, \textcolor{green}{\pi = true})
	\]
	As you can see, \texttt{a} is a paramenter that can be known only at run time, as it is a user-supplied parameter. Therefore, the symbolic store will assign it to the symbolic value $\alpha_a$: this is a way of stating that, according to our current knowledge, \texttt{a} can assume any value in the range of integers, but we don't make any assumptions about it because it's not necessary (yet). $\pi$ is initially set to \textit{true} because we are currently making no assumption about the value of any symbol.
	
	The next step represent what happens after the (abstract) execution of \texttt{int x = 1}. The state is modified as follows, where the coloured elements are those that differ from the previous step:
	\[
	(\textcolor{blue}{\texttt{stmt}}, \textcolor{violet}{\sigma}, \pi) = (\textcolor{blue}{\texttt{bool y = true}}, \textcolor{violet}{\sigma = \{a \mapsto \alpha_a\, x \mapsto 1\}}, \pi = true)
	\] 
	Once the instruction on line 2 is executed, the symbolic store can map the variable \texttt{x} with the \textit{concrete} value 1, which can be inferred statically.
	
	The next instruction involves a branch conditioned on the value of \texttt{a}, which is not known. Therefore, the symbolic execution engine produces two paths, one for $\alpha_a = 0$ and the other for $\alpha_a \neq 0$:
	\[
	(\textcolor{blue}{\texttt{stmt}}, \sigma, \textcolor{green}{\pi}) = (\textcolor{blue}{\texttt{if (a != 0)}}, \sigma = \{a \mapsto \alpha_a\, x \mapsto 1\}, \textcolor{green}{\pi = \{\alpha_a = 0\}})
	\]
	\[
	(\textcolor{blue}{\texttt{stmt}}, \sigma, \textcolor{green}{\pi}) = (\textcolor{blue}{\texttt{if (a != 0)}}, \sigma = \{a \mapsto \alpha_a\, x \mapsto 1\}, \textcolor{green}{\pi = \{\alpha_a \neq 0\}})
	\]
	Through this kind of branching, the symbolic execution evolves as a tree until the program has terminated or until we gained enough knowledge to determine whether the property of interest is satisfied or not.
	\subsubsection{Challenges in symbolic execution}
	The tree structure of a symbolic execution suggests that this kind of analysis suffers of the same issue that we have already encoutered with taint analysis, namely \textbf{path explosion} (or state space explosion): as the size of the program grows, the number of feasible paths grows exponentially, and can even be infinite if the program contains unbounded loop iterations. Although an exhaustive exploration of the state space is the only way to guarantee a sound and complete analysis, a partial exploration is sufficient in many scenarios to prove a certain property, thus the problem of path explosion can often be circumvented.
	
	Another problem of symbolic execution concerns \textbf{constraint solving}. As precisely stated in \cite{Baldoni2018}, "in a symbolic executor, constraint solving plays a crucial role in checking the feasibility of a path, generating assignments to symbolic variables, and verifying assertions". These constraints are expressed in a logical language and their truth value is evaluated by decision procedures called "constraint solvers". 
	Finding a solution for such formulas is notoriously an NP-complete problem: for this reason, one of the most popular approaches to constraint solving consists in mapping the formula to an instance of the boolean satisfiability problem (also known as SAT, see Appendix \ref{appendix:complexity}), a famous NP-complete problem for which many efficient algorithms have been developed. For those problems that cannot be easily mapped to SAT, we can use Satisfiability Modulo Theories (SMT), that generalize the SAT problem with supporting theories to capture more complex formulas. Even though some significant advances has been accomplished in recent years to optimise the search of a solution, complexity remains a major obstacle to the scalability of symbolic execution to large programs. Moreover, not even SMT solvers are powerful enough to handle constraints that involve certain operations, such as non-linear arithmetic, which prevents us from exploiting the advantages of optimised SMT solvers.
	
	\chapter{Spectre}\label{chapter:spectre}
	On January 2018, two major vulnerabilities affecting the vast majority of modern CPUs were disclosed, taking the names of \textit{Spectre} \cite{Kocher2019} and \textit{Meltdown} \cite{Lipp2018}. These two attacks were independently discovered by various researchers and were published in 
	two works, \cite{Kocher2019} and \cite{Lipp2018}, destined to leave a permanent mark in the field of cybersecurity. More precisely, \cite{Kocher2019} identified \textit{Spectre}, a family of attacks that leverage on an optimisation technique used in modern processors to read and write protected memory from a program's address space, while \cite{Lipp2018} describes \textit{Meltdown}, an attack able to bypass the privilege checks that usually prevent a user process to access regions of memory that are under the control of the operating system. Both are classified as \textit{cache side-channel attacks} and were found to affect Intel, ARM and AMD processors, present in the vast majority of desktops, laptops, cloud servers, and even smartphones\footnote{Source: \url{https://spectreattack.com/}}.
	
	This chapter is focused on Spectre, which is classified as an \textit{access-driven cache side-channel attack}, as it is based on the adversary's ability to monitor cache accesses made by the victim and measuring the time difference between a cache access and a memory access.
	
	This chapter provides all the background notions that are needed to comprehend the mechanism of a Spectre attacks. Furthermore, it contains a complete overview of all the variants of the attacks that are currently known, describing what microarchitectural data structure they exploit and how the attack is carried out.
	\section{Background}
	This section deals with all the background concepts that lie at the basis of Spectre, in particular speculative execution and cache side channels. As stated in the pioneering work by Kocher et al. \cite{Kocher2019}, "Spectre attacks violate memory isolation boundaries by combining speculative execution with data exfiltration via microarchitectural covert channels."
	\subsection{Out-of-Order execution and micro-operations}\label{ooo-exec}
	When executing a program, the processor splits single assembly instructions in a series of lower-level operations called \textit{micro-operations} (also abbreviated as micro-ops or $\mu$-ops): this has the advantage of allowing the CPU to execute different part of the instructions in different moments, based on the current availability of the data \cite{Fog2021}. For example, consider the following piece of code, written in Intel syntax:
	\begin{lstlisting}
add eax, ebx
add [reg], eax
	\end{lstlisting}
	The instruction on line 1 adds the content of two cache registers, \texttt{eax} and \texttt{ebx}. This means that the data will be immediately available to the CPU and thus this instruction doesn't need to be split in more than one $\mu$-op. The instruction on line 2, instead, is much different: it requires to (i) retrieve \texttt{[reg]}, namely the data located at the memory address stored in \texttt{reg}, then (ii) add it to the content of \texttt{eax}, and finally (iii) to write the result back to \texttt{[reg]}. Therefore, up to two memory accesses are needed to complete the instruction, but since these accesses are very time-consuming, the CPU splits the instructions into three $\mu$-ops (corresponding to the three step described), so that it can execute other tasks while it waits for the data to be accessible, avoiding to waste hundreds of clock cycles and thus speeding up the computation. This is precisely what is referred to as \textit{out-of-order execution}. This paradigm, however, comes with a challenge: when executing $\mu$-ops out of order, the CPU must establish whether there are dependencies between different instructions that can obstacle the completion of certain tasks.	For example, let's consider again the two assembly instructions written above: note that the addition on line 2 involves \texttt{eax}, which is modified by the previous addition. This means that the micro-operation (ii) can't be executed before the result of the previous addition is not stored in \texttt{eax}. To implement out-of-order execution successfully, the CPU uses a mechanism called \textit{memory disambiguation}, accurately described in \ref{sec:spectre-stl}, to detect and resolve this kind of dependencies. 
	
	\subsection{Speculative execution}\label{sec:speculative-exec}
	Speculative execution is an optimization technique where the CPU uses statistical information about the program execution to make guesses about the outcome of conditional branches or a data dependencies, and consequently decides to load data and execute certain micro-operations in advance. If the prediction is correct, the processor can benefit of the intermediate results and the data that loaded to speed up the execution of the following instructions. Otherwise, the CPU performs a rollback to the last correct state, discarding the intermediate results computed speculatively. It's imporatant to highlight that the effects of speculative execution are visible only at microarchitectural level until they are committed to the architectural state: the user doesn't have the perception that the execution flow is not strictly linear, and never sees the effect of incorrect predictions in the program's output. However, when a misprediction happen, the intermediate results and the data loaded preventively in cache remain there. This asymmetry between the microarchitectural state and what the user sees from the execution of the program gives space to the a type of attack vector known as \textit{cache side channel}, which is discussed in \ref{sec:cache-sc}.
	
	\subsection{Cache side channels}\label{sec:cache-sc}
	A cache side channel \cite{Zhang2017} is a type of attack vector that allows to infer secret variables by monitoring the state of the cache during the program execution. So far, three different kinds of cache side-channels have been identifies, each of which monitors different behaviours of the cache: time-driven \cite{Page2002}, trace-driven \cite{Page2002}, and access-driven.
	\paragraph{Time-driven side channels} In this case, the adversary keeps track of the execution time of the program and use that information to infer what data is loaded into the cache. 
	A typical example of timing attack targets the modular exponentiation function used in many public key cryptographic algorithm, included RSA. Modular exponentiation consists in raising a large number (i.e. the plaintext or the ciphertext) to a large exponent (i.e. the public key or the secret key respectively), which is extremely time consuming, since it requires to perform repeated multiplications using large integers. The subprocedure \texttt{square-and-multiply}, allows to do this computation efficiently:
	\begin{lstlisting}
square-and-multiply(M, e, N){
	R = 1
	for (i=0 to n-1){
		R = R^2 mod N
		if(e[i] == 1){ R = R * M mod N }
	}	
	return R
}
	\end{lstlisting}
	where \texttt{M} is the message we want to encrypt (or decrypt), \texttt{e} is the exponent, \texttt{n} is the its length, \texttt{N} is the modulo, \texttt{R} is the result. As one can see, for each bit of the secret or public key \texttt{e}, the message is squared. If the current bit of the exponent is 1, then an additional multiplication is performed on the result: this means that, when the value of \texttt{e[i]} is 1, the computation on \texttt{R} takes longer to complete. This time difference could potentially be exploited to figure out which bits are set to 1 in the key, even though most systems nowadays employ some countermeasures to prevent it (for example, by adopting constant-time algorithms).
	\paragraph{Trace-driven side channels} In this attack, the adversary monitors the amount of power consumed during the execution. The power trace is a rich source of information, as it can reveal not only when certain operations are performed, but also what data is being used at each stage of the computation. The attacks carried out through this vector are also known as \textit{power analysis} attacks and they proved to be very effective: for example, numerous attacks of this kind have been successfully conducted on AES, the standard algorithm for simmetric encryption, to exfiltrate entire bytes of the symmetric key \cite{Buchanan2017} \cite{Mangard2003} \cite{Mangard2010} \cite{Oswald2004}. The attack is performed collecting a large number of power traces, on which the adversary performs different statistical analyses that can reveal the data dependency between the power consumption and the execution time. This attack style is known as \textit{Differential} Power Analysis (DPA), to distinguish it from \textit{Simple} Power Analysis (SPA), where the attacker needs only one single trace and tries to deduce information about the secret key from it.
	\paragraph{Access-driven side channels} 
	Access-driven attacks, like time-driven ones, leverage on time measurements to unveil the value of some secret information. However, they rely on a much finer kind of measurement: while time-driven attacks are based on the execution time of the whole program, access-driven ones exploit "the ability to
	detect whether a cache line has been evicted, or not, as the primary
	mechanism for mounting an attack" \cite{Neve2007}. From this point of view, this attack requires a much finer measurement capacity. A famous example of how to concretely exploit the timing information is \textsc{Flush$+$Reload} \cite{Yarom2014}, an attack that relies on sharing pages between the attacker and the victim processes. \textsc{Flush$+$Reload} can be summarised through the following steps: the adversary's goal is to verify whether the victim uses a secret piece of data $d$. To accomplish this, the adversary's process evicts the cache line containing $d$ and waits a certain amount of time, to give the victim the opportunity of using $d$, in case they need it. Then the attacker reloads the memory line and measures the time to load it: if the victim accessed $d$ during the latency time, the reload will be very quick, because $d$ was already brought back to the cache. Otherwise, $d$ is still in the main memory and the reloading time will be significantly longer, indicating that the victim did not access $d$ during the latency. Figure \ref{fig:flush-reload}, taken from the paper that first described the attack \cite{Yarom2014}, can help clarifying these concepts.
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.3]{../imgs/flush+reload}
		\captionsetup{width=.8\linewidth}
		\caption{\textsc{Flush+Reload} time measurement \cite{Yarom2014}. (A) and (B) represent the main cases described above. (C), (D), (E) are more peculiar cases.}
		\label{fig:flush-reload}
	\end{figure}
	One of the assumptions that must hold to carry out such attacks successfully is that the adversary must share the cache space with the victim.
	
	\section{Spectre variants}\label{sec:spectre-var}
	As mentioned at the beginning of this chapter, the term "Spectre" on itself does not identify a single attack, but a \textit{family} of attacks, all united by the same common denominator: they maliciously influence the speculative execution of a program to exfiltrate confidential information. Different version of Spectre can be carried out through the exploitation of different microarchitectural behaviours. The purpose of this section is to present the four main variants of Spectre, describing what vulnerability they take advantage of and in how they differ from each other.
	\subsection{Spectre-PHT (Pattern History Table)}
	Spectre-PHT \cite{Kocher2019}\cite{Canella2019}\cite{Evtyushkin2018}, also known as variant 1 or Bound Check Bypass, was one the first attack of the Spectre family to be discovered. As the name suggests, this version targets the Pattern History Table (PHT) to trigger a branch misprection.
	\paragraph{Pattern History Table (PHT)} The Pattern History Table \cite{Fog2021} is a data structure used by the branch predictor to guess the outcome of a conditional branch before the value of the guard is fully determined. Intuitively, during the execution of a program, the CPU keeps track of the last $N$ outcomes of a conditional branch in the \textit{branch history register}. The outcomes are encoded as a bitstring of length $N$, where a 1 indicates an execution where the guard expression evaluated to true, while a 0 represents the opposite case. The content of the branch history buffer is used to point at a specific entry of the Pattern History Table. The PHT contains $2^N$ entries, i.e. as many entries as all the possible sequences of $N$ outcomes; each entry in
	the contains a 2-bit saturating counter, namely a finite-state automaton that decides the probability of a certain outcome. The branch history register is used for choosing which of the four counters to use.
	
	\paragraph{The attack} By poisoning the PHT, the adversary can induce the branch predictor to make a wrong guess about the direction of the conditional branch and perform both reading and writing operations in memory parts they shouldn't have the right to access. To clarify this concept, I propose a couple of examples, taken from \cite{Canella2019}:
	\begin{lstlisting}
if(x < len(array1)) {
	y = array2[array1[x] * 4096]
}
	\end{lstlisting}
	Note that the index we use to access \texttt{array2} is multiplied by 4096. The reason is that the usual cache block size is 64 bytes, so by using indexes in the form \texttt{[k * 4096]} we avoid having two elements used in the program falling into the same cache block.
	The conditional instruction allows reading a value from \texttt{array2} only if \texttt{x} represents a valid index. However, after repeatedly executing the conditional branch with a value of \texttt{x} that satisfies the guard, the branch predictor will forecast that the expression \texttt{x < len(array1)} evaluates to true and thus will execute the assignment on line 2 in advance. When the adversary supplies an invalid value for \texttt{x}, the CPU will performe an \textit{out-of-bounds memory access}, loading in the cache a value for \texttt{y} that shouldn't be accessed.
	
	The same effect can be exploited to perform a write operation on the array, as shown in the following piece of code:
	\begin{lstlisting}
if (x < len(array)) { 
	array[x] = value; 
}
	\end{lstlisting}  
	Just as the described above, the attacker can "train" the CPU to guess a certain outcome of the branching instruction and then exploits the misprection caused by the provision of an invalid value of \texttt{x} to write on a protected location.
	\subsection{Spectre-BTB (Branch Target Buffer)}
	Spectre-BTB \cite{Kocher2019} \cite{Canella2019}, also known as variant 2 or Branch Target Injection, affects the Branch Target Buffer to deflect the transient execution to a mispredicted branch target. This attack was discovered simultaneously to variant 1 and exploits a similar mechanism.
	
	\paragraph{Branch Target Buffer (BTB)} The Branch Target Buffer \cite{Perleberg1989} is a data structure located in the cache, that stores a set of guesses for the target addresses of all jumps, both conditional and unconditional. The first time a jump instruction is executed, the target address that is reached gets stored in the BTB, so no speculation is made for the first jump. When the jump is executed again, a pointer to the BTB indicates the target address of the previous execution, allowing the CPU to fetch the  predicted instruction into the pipeline, but the true target will not be calculated until the jump reaches the execution stage. Once the jump is actually performed,the address predicted with the BTB is compared with the actual address taken by the jump: if they don't match, the guess was wrong, so the results are rolled back and the previous target address is replaced by the current one in the BTB.
	\paragraph{The attack} The logic that governs this variant is very akin to the one behind Spectre-PHT: both attacks target data structures that aim to anticipate the outcome of jump instructions, trying to influence the prediction about what is executed as a consequence of the jump.
	
	However, there is a significant difference between Spectre-PHT and Spectre-BTB: in the former, the execution flows along a restricted mispredicted path, i.e. the attacker can influence the branch predictor only in the choice of the branch to execute. The latter, instead, allows redirecting the control flow to an arbitrary destination, so that the execution can continue at a specific point chosen by the adversary. This location corresponds to a \textit{Spectre gadget}, namely "a code fragment whose speculative execution will transfer the victim’s sensitive information into a covert channel"\cite{Kocher2019}.
	
	\subsection{Spectre-RSB (Return Stack Buffer)}
	Spectre-RSB \cite{Maisuradze2018} \cite{Koruyeh2018} \cite{Canella2019}, also known as ret2spec, is a variant that exploits a data structure called Return Stack Buffer.
	
	\paragraph{Return Stack Buffer (RSB)}The Return Stack Buffer is a microarchitectural buffer used to predict return address of a function: whenever a call instruction is reached during the execution, the prediction of the return address is pushed on top of a stack. When the execution reaches the \texttt{return} point, the top entry of the stack is used to speculate about the return address location quickly. Meanwhile, the actual return address is fetched, possibly from the main memory, therefore it will be available after only after hundreds of clock cycles. Once the real return address is loaded, it gets compared with the address that was fetched from the RSB: if they match, all the result computed until that point can be committed and the overall execution time gains in speed.
	
	\paragraph{The attack} To describe Spectre-RSB attacks, we also refer to another kind of stack: the \textit{program stack}, namely a data structure that stores information about the active subroutines of a computer program.
	
	There are various ways in which the adversary can poison the RSB, all described in detail in \cite{Koruyeh2018}. One way to carry out the attack is to exploit the stack overfill or underfill: the RSB has a very limited size, which can vary between 4 and 24 entries, thus it saturates quickly. When this happens, the stack gets updated in a cyclic manner, namely the latest return address is pushed on the stack, the $n$-th entry is discarded and the $(n-1)$-th takes its place. As the functions progressively reach the \texttt{return} instruction, the entries of both the program stack and the RSB get popped, but at some point we reach the function whose value has been overwritten, causing an underfill of the RSB.
	
	The primary attack strategy is to directly pollute the RSB: the adversary can overwrite the return address on the program stack, so that the top entry represents the return address of the previous function. In this way, the address found on the program stack and the on top of the RSB will certainly mismatch. 
	
	Another way is to leverage on the \textit{speculative} pollution of the RSB. When functions are called during a speculative execution and a misspeculation happens, their results are rolled back and they are removed from the stack. However, the guessed return addresses remain in the RSB, providing the opportunity to push an address that points outside the address space accessible by the program without raising exceptions.
	
	\subsection{Spectre-STL (Store To Load)}\label{sec:spectre-stl}
	Spectre-STL \cite{Canella2019}, also known as variant 4 or Speculative Store Bypass, is a variant that exploits not only dependencies in the control flow, but also those in in the \textit{data flow}. More precisely, this version takes advantage of the memory disambiguation mechanism that is put in place by most moder processors.
	
	\paragraph{Memory disambiguation} Memory disambiguation\footnote{\url{https://en.wikipedia.org/wiki/Memory_disambiguation}} is a set of techniques used to execute memory access instructions out of order, without affecting the value of the final result. To justify the need of these techinques, I will present two examples, both taken from (Wikipedia):
	
	\begin{lstlisting}
add $1, $2, $3      # R1 <= R2 + R3
add $5, $1, $4      # R5 <= R1 + R4
	\end{lstlisting}
	
	The code above shows two micro-operations that perform simple additions. The result of the second instruction depends on the result of the first one, since the value of register R1 is computed in line 1 and then R1 is used as operand for the addition in line 2. This is a case of \textit{static} dependence, because the sources and destinations of the operations are registers. The processor can easily spot this dependence and decide an order of execution where the first addition is performed before the second one, so that the result is consistent with a sequential execution.
	
	However, "complications arise when the dependence is not statically determinable". Consider the following code snippet:
	
	\begin{lstlisting}
store $1, 2($2)      # Mem[R2+2] <= R1
load  $3, 4($4)      # R3 <= Mem[R4+4]
	\end{lstlisting}
	In this case, the location of the operand is indirectly specified by means of a register, rather than directly defined in the instruction encoding itself. As clearly stated \href{https://en.wikipedia.org/wiki/Memory_disambiguation}{here}, "the microprocessor cannot statically determine, prior to execution, if the memory locations specified in these two instructions are different, or are the same location, because the locations depend on the values in R2 and R4". As a consequence, it is not possible to determine at compile time whether these two instructions can be executed in a different order or not: this is known as \textit{ambiguous} dependence. Detecting and resolving ambiguous dependencies require more sophisticated techniques than static dependency, and this is where the memory disambiguation mechanism comes into play.
	
	To further improve the performances, some processor support a technique called \textbf{memory dependence prediction}, which is analogous to branch prediction, that leaves room for a Spectre attack. This method aims at predicting the true dependencies between store and loads, in order to speculatively execute certain memory accesses out of order without affecting the final result of the computation. Similarly to the other variants, the guesses of the memory dependence predictor must be validated or discarded later in the pipeline, when the memory disambiguation system takes action and determines whether the loads and store where correctly executed.
	\paragraph{The attack} Spectre-STL exploits the prediction mechanisms to read a value from a protected address. The core idea of the attack leverages on the saw called \textit{RAW hazard}: "RAW" stands for "Read-After-Write" and it indicates a data dependency where a load operation reads a value from a memory location that was subjected to a store operation in a previous instruction. A RAW-hazard takes place when the processor reads the address \textit{before} the previous store operation commits its value to the memory.
	
	The following example clarifies how to exploit a RAW-hazard to carry out a Spectre attack:
	\begin{lstlisting}
ptr = secret_pointer;		// initial value
ptr = public_pointer;		// STORE
if(is_public(ptr)){	
	value = *ptr;			// LOAD
}
cache = array[value];		// look-up
	\end{lstlisting}
	On line 1, the value of the pointer gets initialised with the location of a secret value, while on line 2 it gets re-assigned to the location of a public value. The \texttt{if} branch checks whether the pointer corresponds to a public address, in which case it allows to read the value stored there. The attack succeeds if the adversary can induce the CPU to delay the commitment of the second store operation, executing the \texttt{if} branch with the initial value of \texttt{ptr}.
	
	Note that this variant does not involve manipulating the branch predictor in any way: throughout the attack, the \texttt{if} clause always evaluates to \texttt{true} (explain better).
	
	\chapter{Verification tools to detect Spectre}\label{chapter:verification}
	
	\textit{Formal verification} consists in checking whether a program implements a given specification, i.e. the program is functionally correct. \textit{Formal analysis}, on the other hand, is used to verify whether the code satisfies certain properties, e.g. specific security guarantees. In our case, the security property that we want to target is the vulnerability against Spectre-like attacks. 
	
	This work aims at comparing two different tools: on one hand we have FastSpec \cite{Tol2021}, a recent tool that uses deep learning techniques not only to detect Spectre, but also to generate new gadgets using a GAN-based framework named SpectreGAN. On the other hand we have KLEESpectre \cite{Wang2020}, a tool that employs symbolic execution extended with cache modelling and speculative execution to detect Spectre vulnerabilities in the code. The core of this work consists in testing the security of symmetric ciphers in OpenSSL\footnote{Official website: \url{https://www.openssl.org/}}, a popular software library used to ensure secure communication over the network. 
	More precisely, we tackle the following questions: 
	\begin{enumerate}
		\item Do FastSpec and KLEESpectre detect any Spectre vulnerability? If so, what variant do they detect?
		\item Do FastSpec and KLEESpectre provide the same results?
		\item Comparing the results obtained on hash functions implemented in OpenSSL v1.0.0 (last revised in 2015, before the discovering of Spectre) and those implemented in OpenSSL v3.0.0 (the latest version), are there any differences? 
	\end{enumerate}
	As specified in \cite{Tol2021}, FastSpec was already used to test OpenSSL v3.0.0 on public-key algorithms and procedures for digital signatures, namely RSA, EDCSA and DSA. Our work targets one specific symmetric-key cipher, AES (Advanced Encryption Standard, see Appendix \ref{appendix:aes}), in different modes of operation. 
	
	The rest of the chapter is organised as follows: Section \ref{sec:fastspec} and \ref{sec:kleespectre} provide an overview on SpectreGAN and KLEESpectre, describing their main features and analysis strategies. Then we proceed with Section \ref{sec:testing}, where we present the experimental results obtained by testing the OpenSSL functions both with FastSpec and KLEESpectre. 

	\section{FastSpec}\label{sec:fastspec}
	SpectreGAN is a Generative Adversarial Network that was presented along with FastSpec \cite{Tol2021}, a tool based on deep learning that is able to identify Spectre gadgets much faster than common rule-based methods. The need to employ deep learning techniques comes from a significant limitation encountered in other analysis tools: the detection of vulnerable code sections often relies on a pattern matching process that compares these sections with known gadgets, which amount to only 17 (15 by Kocher \cite{Kocher2018} and 2 modified example introduced by Spectector). Despite being conceptually sound, this strategy falls short at recognising subtle code variations occuring in different programs. Furthermore, such a low number of gadgets makes it difficult to perform a comprehensive analysis of the tools.
	
	The authors of FastSpec target this problem through the following steps:
	\begin{itemize}
		\item They extend the current set of 15 gadgets to 1 million by applying a technique called \textit{mutational fuzzing}, creating an appropriate dataset for the deep learning algorithm;
		\item They describe a specific implementation of a GAN, named \textit{SpectreGAN}, that is trained on the dataset mentioned in the preovious point to learn the distribution of existing Spectre gadgets;
		\item They introduce \textit{FastSpec}, a tool based on supervised neural network embeddings, to identify potential gadgets faster that rule-based methods.
	\end{itemize}
	The rest of the section is focused on the first two points, namely in the parts that deal with the generation of new gadgets, as FastSpec is out of our scope of interest. All the background notions about the deep learning techniques used within this tool are discussed in Appendix \ref{appendix:dl}.

	The authors propose two ways of producing new gadgets starting from the initial set of 17: the first one leverages on random insertion of instructions, and is used to create a large dataset of code snippets. The second one uses the previously generated dataset to train a Generative Adversarial Network: the goal is to learn the leakage distribution of Spectre vulnerabilities, to be able to "intelligently" produce vulnerable code.
	\paragraph{Mutational fuzzing}
	Firstly, the initial set of gadgets is increased by compiling each code snippet with three different compilers and different optimisation options, obtaining 6 assembly functions for each gadget. Then, the authors describe an algorithm that performs the following steps on each gadget:
	\begin{enumerate}
		\item It inserts random assembly instructions in random locations of the gadget,
		\item It checks whether the functional properties of the gadgets are preserved,
		\item It checks whether the new gadget still leaks secrets through speculative execution.
	\end{enumerate}
	Step 2) is particularly imporant because it verifies whether the newly inserted instructions improperly modify the flags that are checked in the conditional jumps: if that is the case, the secret may be leaked without even exploiting speculative execution.
	\paragraph{SpectreGAN}
	As the authors clarify in their work, "\textit{the purpose of SpectreGAN is to develop an intelligent way of creating assembly functions instead of randomly inserting instructions and operands}" \cite{Tol2021}. In this way, they don't simply modify the gadgets with arbitrary changes, but they build a system that learns the features that characterise a vulnerable piece of code and is able to reproduce them in a brand new code snippet.
	
	Both the Generator and the Discriminator are implemented with seq2seq, an architecture commonly used for machine translation tasks (see Appendix \ref{appendix:dl}). As for the \textbf{generation phase}, a gadget is sampled from the dataset, divided in tokens, and "masked", i.e. some arbitrary tokens in the gadget are hidden. The Generator learns the leaking patterns by trying to fill correctly the masked positions. As for the \textbf{discrimination phase}, the Discriminator is fed with 1) the embedding vectors corresponding to the original tokens, and 2) the embedding vectors corresponding to the \textit{masked} tokens ...
	
	
	\section{KLEESpectre}\label{sec:kleespectre}
	KLEESpectre \cite{Wang2019} leverages on symbolic execution (see Section \ref{sec:speculative-exec}) and modelling of cache accesses to perform the analysis of a program. As the name suggests, this tool was obtained by modifying the symbolic execution engine KLEE \cite{Cadar2008} to model the branch speculation that are the root of Spectre vulnerabilities. 
	\paragraph{Speculative symbolic execution} As already discussed in Section \ref{sec:symbolic-exec}, standard symbolic execution analyses code by representing the different paths that the program can take and abstracting the unknown inputs with symbolic values. For example, consider the following piece of code shown in Figure ... A standard symbolic execution engine like KLEE would explore the paths shown in Figure \ref{fig:standars_symb_exec} : since \texttt{x} is a user-supplied value, and therefore cannot be determined statically, the KLEE would model the two possible outcome for the first conditional statement, on line 5: either the branch condition on \texttt{x} is true ($p_{T1}$) or it is false ($p_{F1}$). The same reasoning is made for the second conditional statement, on line 8.
	\begin{lstlisting}[escapechar=!]
uint32_t SIZE = 16; 
uint8_t array1[16], array2[256*64], array3[16];
uint8_t foo(uint32_t x) { 
	uint8_t temp = 0; 
	!\colorbox{lightpurple}{if (x < SIZE)}! {		  // branch b1
		!\colorbox{lightblue}{temp = array1[x];}!
		!\colorbox{lightblue}{temp |= array2[temp];}!
		!\colorbox{lightgreen}{if (x <= 8)}! {		// branch b2
			!\colorbox{yellow}{temp |= array2[8];}!
		}	
	}
	!\colorbox{pink}{temp |= array3[8];}!
	!\colorbox{pink}{return temp;}!
}
	\end{lstlisting}

	\begin{figure}[!h]
		\centering
		\includegraphics[scale=0.3]{../imgs/symb_exec}
		\caption{Standard symbolic execution of the program.}
		\label{fig:standars_symb_exec}
	\end{figure}
	KLEESpectre, instead, makes a step further, considering four different scenarios for each branching instruction. For instance, the execution of the first branch is modelled as follows: 
	\begin{enumerate}
		\item $p_{T1}$: \texttt{x < SIZE} is satisfiable and the branch \texttt{b1} is \textcolor{green}{correctly predicted}. In this case, the symbolic execution will fork a new state with constraint \texttt{x < SIZE} and proceeds by executing the code fragment highlighted in \colorbox{lightblue}{blue}.
		\item $p_{F1}$: \texttt{x >= SIZE} is satisfiable and the branch \texttt{b1} is \textcolor{green}{correctly predicted}. In this case, the symbolic execution will fork a new state with constraint \texttt{x >= SIZE} and proceeds by executing the code fragment highlighted in \colorbox{pink}{pink}.
	\end{enumerate}
	These are the two cases that are also considered by a standard symbolic execution engine, to which two more are added:
	\begin{enumerate}[resume]
		\item $sp_{T1}$: \texttt{x >= SIZE} is satisfiable and the branch \texttt{b1} is \textcolor{red}{mispredicted}. In this case, KLEESpectre forks a new state with constraint \texttt{x >= SIZE}, but proceeds by executing the code fragment highlighted in \colorbox{lightblue}{blue}.
		\item $sp_{F1}$: \texttt{x < SIZE} is satisfiable and the branch \texttt{b1} is \textcolor{red}{mispredicted}. KLEESpectre forks a new state with constraint \texttt{x < SIZE}, but proceeds by executing the code fragment highlighted in \colorbox{pink}{pink}.
	\end{enumerate}
	The decision tree produced by KLEESpectre as depicted in Figure \ref{fig:spec_symb_exec}
	
	\begin{figure}[!h]
		\centering
		\includegraphics[scale=0.3]{../imgs/spec_symb_exec}
		\caption{Speculative symbolic execution of the program.}
		\label{fig:spec_symb_exec}
	\end{figure}
	
	The critical scenario, namely the one that poses a security threat, is the third one: the processor executes the instructions contained in the scope of the branching statement, even though the condition is not satisfied. 
	
	KLEESpectre is also able to handle the problem of state space explosion, by discarding any path that do not pose any risk of data leakage. In the example shown in Figure \ref{fig:spec_symb_exec} the symbolic states $sp_{T3}$,$sp_{F3}$, $sp_{T4}$ and $sp_{F4}$ are all discarded once KLEESpectre reaches the limit of speculative execution window (SEW, see Section \ref{sec:taint}). 
	\paragraph{Cache modelling} Speculative symbolic execution is further extended with cache modelling: in simple terms, this means that KLEESpectre models the cache behaviour of an execution path (concrete or speculative) to verify whether a secret value remains in the cache when the program terminates. 
	
	To do this, KLEESpectre distinguishes between two different types of access:
	\begin{itemize}
		\item \textit{concrete} accesses, made to a known memory address;
		\item \textit{symbolic} accesses, made to an address that can be determined only at run time, and thus must be abstracted with a symbolic value. 
	\end{itemize}
	
	\section{Testing AES}\label{sec:testing}
	\appendix
	\chapter{Complexity theory}\label{appendix:complexity}
	\section{Complexity classes}
	\section{Boolean satisfiability problem: SAT}
	\chapter{Deep Learning}\label{appendix:dl}
	\section{Generative Adversarial Networks (GANs)}
	The Generative Adversarial Network \cite{Goodfellow2014} is a learning framework designed for the task of \textit{generative modelling}, an unsupervised learning task that involves the automatic discovering and learning of patterns in data distributions in such a way that the model can generate new examples that plausibly could have been drawn from the original data set. The innovative idea behind GANs lies in the learning strategy, that sees two actors involved:
	\begin{itemize}
		\item a \textbf{Generative} net, $G$, that generates data;
		\item a \textbf{Discriminator} net, $D$, that evaluates these data.
	\end{itemize}
	Intuitively, the Discriminator should be able to distinguish authentic data from artificially generated data. The goal of the Generator is to produce, starting from a random data distribution, increasingly better data that will eventually fool the Discriminator. From a game theory point of view, the convergence of a GAN is reached when the generator and the discriminator reach a \textit{Nash equilibrium} \cite{Ratliff2013}.
	\paragraph{Generator}
	To produce a certain kind of data, we assume that there exists a probability distribution (referred to as \textbf{target distribution}, $p_t$) that describes that data we want to create: the points of the distribution represent vectors of features that characterize that type of data. The goal of the Generator is to be able to sample a random noise, $z$, from a normal or uniform distribution (denoted by $p_g$) and turn it into a point $x$  following the target distribution, through a function denoted by $Gen()$:
	\[
	x = Gen(z) \text{ with } z \sim p_g
	\]
	
	Conceptually, $z$ represents the latent features of the images generated, for example, the colour or the shape. In fact, the space from which the Generator samples the seed z is also referred to as \textbf{latent space}.
	\paragraph{Discriminator}
	The Generator, alone, can only produce some random noise: the purpose of the Discriminator is to guide the Generator to create data that look more similar to the real ones. It's fundamental to underline that before the beginning of the adversarial exchange between the two nets, the Discriminator must be trained (in a supervised fashion) with a data set of real and fake data.
	For each point in the training set, the net outputs the value $D(x)$, representing the probability that $x$ is real. Thus, what we \textit{ideally} want to obtain at the end of the training is that:
	\[
	D(x) = \begin{cases}
		1 &\text{ if the input is real }\\
		0 & \text{ otherwise }
	\end{cases}
	\]
	Concretely, during the training, the goal is to \textit{maximize} the ability of discerning real data from fake ones:
	\[
	\max_D V(D) = \mathbb{E}_{x\sim p_t(x)}[\log D(x)] + \mathbb{E}_{z\sim p_z(z)}[1-(\log D(G(z)))]
	\]
	where $\mathbb{E}$ denotes the expected value.
	
	To measure the loss, it is possible to use \textbf{cross-entropy}, the most popular loss function for learning algorithms that use gradient descent.
	\paragraph{Adversarial learning}
	Once the Discriminator is properly trained, the Generator enters into play, and the adversarial confrontation begins. The goal of the Generator is to minimize the difference between the data it generates and real data:
	\[
	\min_G V(G) = \mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]
	\]
	Overall, the two nets play a so-called \textbf{minimax game}:
	\[
	\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_t(x)}[\log D(x)] + \mathbb{E}_{z\sim p_z(z)}[\log (1-D(G(z)))]
	\]
	where $V(D,G)$ represents the value function.
	
	The last equation makes it clear why this style of training is called \textit{adversarial}: while the Generator tries to minimise a function, the Discriminator tries to maximise it, thus the two networks have opposite goals. (maybe explain better..?)
	
	\section{Natural Language Processing (NLP)}
	\chapter{Block ciphers and AES}\label{appendix:aes}
	The goal of this appendix is to provide the basic notions concerning block ciphers and modes of operations that are fundamental to understand how AES works. Given the breadth of the topic, here we will discuss only the concepts that are strictly necessary to understand the core of this thesis, but the curious readers can rely on the references given in the rest of the appendix to dig deeper into the topic.
	
	The appendix begins by presenting some game-based security notions, necessary to understand how the security of ciphers is evaluated. Then we briefly describe block ciphers in Section \ref{sec:block-ciphers}, before proceeding with Section \ref{sec:modes}, where we present how a block cipher can be used repeatedly on different part of the message to produce the final encryption. Finally, we have all the tools to reach Section \ref{sec:aes}, concerning the history and the design of AES.
	
	\section{Block ciphers}\label{sec:block-ciphers}
	A block cipher is a deterministic algorithm $F$ that operates on data units of fixed length, called \textit{blocks}. $F$ accepts in input a data block of size $n$, and performs a transformation using a key $k$, producing in output a block of same size as the input. More formally, $F$ is defined as follows:
	\[
		F: \{0,1\}^{|k|} \times \{0,1\}^n \rightarrow \{0,1\}^n
	\]
	where $|k|$ represents the key length and $n$ the size of the block. The definition given above means that a ciphertext $c$ is obtained from a key $k$ and a message $m$ (also called "plaintext") as:
	\[
		F(k,m) = c
	\]
	Since the key remains fixed for a certain number of encryptions, before being refreshed, we can parametrise $F$ on $k$, defining $F_k(m) \defeq F(k,m)$.
	
	If $F$ represents the encryption algorithm, then the decryption algorithm is obtained simply by computing $F^{-1}$, thus:
	\[
		F^{-1}(k,c) = m
	\]

	\section{Modes of operation}\label{sec:modes}
	There is a clear limitation of block ciphers: they can only encrypt data of fixed size. While the case in which $|m| < n$ can be easily circumvented through padding, if $|m| > n$ we would be forced to discard a part of the message. \textit{Modes of operation} address this problem, providing a way to securely and efficiently encrypt long messages using block ciphers. In simple terms, a mode of operation defines how to concatenate the use of a block cipher in such a way that each cipher encrypts a specific part of the message. In general, if the message has size $|m| > n$, then $m$ is split in $\ell = \frac{|m|}{n}$ different blocks, $m_1$, $m_2$, ... , $m_{\ell}$, and the whole ciphertext is obtained as:
	\[
	c = \langle F(k, m_1), F(k, m_2), ..., F(k, m_{\ell})\rangle
	\]
	where the operator $\langle \cdot , ..., \cdot \rangle$ represents a a generic combination. For the sake of brevity, the rest of this section will present only the modes of operation that were tested in this thesis. Those who are interested in other modes can find them in the references provided throughout this Appendix.
	\subsection{Electronic Code Book (ECB) mode}
	This is the simplest mode of operation: it consists in performing the encryption of the data blocks separately, and then concatenating the results, as shown in Figure \ref{fig:ecb}.
	\begin{figure}[!h]
		\centering
		\includegraphics[scale=0.4]{../imgs/ecb}
		\captionsetup{width=.55\linewidth}
		\caption{Visual representation of Electronic Code Book (ECB) mode (source: \cite{Katz2007})}
		\label{fig:ecb}
	\end{figure}
	Therefore, the whole ciphertext is computed as
	\[
	C = F(k, m_1)\hspace{5px}||\hspace{5px}F(k, m_2)\hspace{5px}||\hspace{5px}... \hspace{5px}||\hspace{5px}F(k, m_{\ell})
	\]
	where the operator $||$ represents a concatenation.
	
	This mode of operation is not used in real-world applications: since it's deterministic (i.e. by encrypting the same plaintext multiple times, we always obtain the same ciphertext), it can be proved that it doesn't satisfy the minimum standards of security. For instance, let's consider the \textit{indistinguishability} property of ciphertexts, which is informally defined as follows:
	\begin{defn}[Indistinguishability]
		Two encryptions are said to be \textit{indistinguishable} if it's infeasible for the adversary to distinguish pairs of ciphertexts based on the message they encrypt.
	\end{defn}
	Since the ECB mode of operation is deterministic, this property is trivially broken, making ECB intrinsecally insecure. Providing a formal argument of why it is considered "insecure" is out of the scope of our work, but the interested readers can refer to \cite{Katz2007} (Section 3.6.2 "Block-Cipher Modes of Operation"), \cite{Menezes2018}, \cite{Schneier2015} for a detailed discussion on the topic.
	\subsection{Cipher Block Chaining (CBC) mode}
	To encrypt using this mode, we choose uniformly at random an initialization vector ($IV$) of length $n$, which is unique for every encryption. Then, we generate the ciphertext blocks by applying the block cipher to the XOR of the current plaintext block and the previous ciphertext block. Figure \ref{fig:cbc} visually depicts the process.
	\begin{figure}[!ht]
		\centering
		\includegraphics[scale=0.4]{../imgs/cbc}
		\captionsetup{width=.55\linewidth}
		\caption{Visual representation of Cipher Block Chaining (CBC) mode (source: \cite{Katz2007})}
		\label{fig:cbc}
	\end{figure}
	Mathematically, the ciphertext is the result of the following operations:
	\[
	\begin{cases}
		c_1 = F_k(IV \oplus m_1)\\
		c_i = F_k(c_{i-1}\oplus m_i) \text{ for all } i>1
	\end{cases}
	\]
	This mode of operation is not deterministic, because the initialisation vector introduces an element of randomness in the encryption. In contrast to the ECB mode, this means that the repeated encryption of the same plaintext results in different ciphertext. This characteristic allows the CBC mode to satisfy the basic standards of security (more precisely, it is CPA-secure\footnote{\url{https://en.wikipedia.org/wiki/Chosen-plaintext\_attack}}). Clearly, the choice of the $IV$ plays a crucial role in the encryption: in order to ensure the indistinguishability of ciphertexts, it's fundamental to choose it uniformly at random. It's also important to note that the $IV$ must be included in the ciphertext to allow a correct decryption.
	
	Despite being secure, this mode of operation presents a mild disadvantage: since the encryption of block $i$ depends on the value of the previous ciphertext, the computation of the final ciphertext can't be parallelised, thus it's not the most efficient choice.
 	\subsection{Output Feedback (OFB) mode}
 	The Output FeedBack (OFB) mode works similarly to the CBC mode, performing some operations in a different order. Figure \ref{fig:ofb} is a high-level depiction of how the OFB mode works.
 	\begin{figure}[!ht]
 		\centering
 		\includegraphics[scale=0.4]{../imgs/ofb}
 		\captionsetup{width=.55\linewidth}
 		\caption{Visual representation of Output Feedback (OFB) mode mode (source: \cite{Katz2007})}
 		\label{fig:ofb}
 	\end{figure}
 	This mode of operation, like Cipher Block Chaining, involves a random initialisation vector, whose value must be unpredictable to guarantee the minimum security requirements.
 	
 	Mathematically, the ciphertext is computed as follows:
 	\[
 	\begin{cases}
 		c_1 = F_k(IV) \oplus m_1\\
 		c_i = F_k(c_{i-1}) \oplus m_i \text{ for all } i>1
 	\end{cases}
 	\]
 	This mode of operation presents the same disadvantage of the CBC-mode, because the computation can't be parallelised.
	\subsection{Counter (CTR) mode}
	In the CTR mode of operation, a counter $ctr \in \{0,1\}^n$ is sampled uniformely at random. Then, the encryption unfolds as follows:
	\[
		c_i = F_k(ctr + i) \oplus m_1
	\]
	where $ctr$ and $i$ are viewed as integers and addition is done modulo $2^n$. Figure \ref{fig:ctr} depicts In contrast to all the modes discussed previously, the CTR mode has the advantage that encryption and decryption can be fully parallelised, since all the blocks can be computed independently of each other.
	\begin{figure}[!ht]
		\centering
		\includegraphics[scale=0.4]{../imgs/ctr}
		\captionsetup{width=.55\linewidth}
		\caption{Visual representation of Counter (CTR) mode mode (source: \cite{Katz2007})}
		\label{fig:ctr}
	\end{figure}
	
	\section{AES: Advanced Encryption Standard}\label{sec:aes}
	AES \cite{Katz2007} \cite{AES-FIPS} is a block cipher that was born in 1998 as replacement of the obsolete DES (Data Encryption Standard) \cite{DES1999}, on request of the U.S. government. At first, its use was limited to military purposes, but nowadays it’s one of the most common block ciphers in use, deeply embedded in various applications. 
	
	The AES cipher has a 128-bit block length and can use 128-, 192-, or 256-bit keys; depending on the key length, the algorithm performes 10, 12 or 14 rounds respectively. During the execution, AES works on a $4\times 4$ array of bytes, called \textit{state}, and each step of the algorithm performs a certain trasformation on this state. More precisely, the main operations present in the algorithm are:
	\begin{enumerate}
		\item \textsf{KeyExpansion}: Derives fresh 128-bit key at each round, using the AES key schedule.
		\item \textsf{AddRoundKey}: Combine each byte of the $4\times 4$ state matrix with a byte of the selected key using the XOR ($\oplus$) operation.
		\item \textsf{SubBytes}: Performs a non-linear substitution where each byte is replaced with another according to a lookup table.
		\item \textsf{ShiftRows}: Performs a transposition where the last three rows of the state are shifted cyclically of a certain number of steps.
		\item \textsf{MixColumns}: Performs a linear mixing operation which operates on the columns of the state, combining the four bytes in each column.
	\end{enumerate}
	Each operation is executed multiple times, according to the order represented in Algorithm \ref{alg:aes}. In this algorithm, we denote with $nRounds$ the number of rounds executed according to the key length, with $K$ the key derived from the AES key schedule at each round, with $S$ the state during each step of the execution, with $SBox$ the non-linear transformation performed during SubBytes, with $nRows$ the amount of shift in the ShiftRow operation, and with $m$ the invertible linear transformation used in MixColumns.
	\begin{algorithm}[H]
		\caption{Advanced Encryption Standard: AES}
		\label{alg:aes}
		\begin{algorithmic}[1]
			\For{$i=0$ to $nRounds$}
			\State $K$ $\leftarrow$ KeyExpansion()
			\State $S$ $\leftarrow$ AddRoundKey($K$)
			\State $S$ $\leftarrow$ SubBytes($Sbox$)
			\State $S$ $\leftarrow$ ShiftRows($nRows$)
			\State $S$ $\leftarrow$ MixColumns($m$)
			\State $S$ $\leftarrow$ AddRoundKey($K$)
			\EndFor
			\State $S$ $\leftarrow$ SubBytes($Sbox$)
			\State $S$ $\leftarrow$ ShiftRows($nRows$)
			\State $S$ $\leftarrow$ MixColumns($m$)
		\end{algorithmic}
	\end{algorithm}
	\bibliography{thesis_ref}{}
	\bibliographystyle{acm}
\end{document}